{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd794f4c",
   "metadata": {},
   "source": [
    "# UK Online Retail EDA (2009–2011)\n",
    "\n",
    "This notebook follows the instructor’s **Step-by-Step Tasks**. By request, we start with:\n",
    "**1) Load & union the data** (no separate project-setup section).\n",
    "\n",
    "> Edit the config cell below if you want to change file paths or export folders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c9f35f",
   "metadata": {},
   "source": [
    "### Config (edit as needed)\n",
    "\n",
    "- Set file paths for the two CSVs.\n",
    "- Choose where to save figures and small derived tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e997bd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import warnings, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "\n",
    "# Display / reproducibility\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "pd.options.display.float_format = lambda x: f\"{x:,.2f}\"\n",
    "\n",
    "# ---- EDIT THESE PATHS IF NEEDED ----\n",
    "DATA_FILES = [\n",
    "    (\"/mnt/data/online_retail_II.xlsx - Year 2009-2010.csv\", \"2009-2010\"),\n",
    "    (\"/mnt/data/online_retail_II.xlsx - Year 2010-2011.csv\", \"2010-2011\"),\n",
    "]\n",
    "FIG_DIR  = Path(\"figures\")   # will be created on first save\n",
    "DATA_DIR = Path(\"data\")      # will be created on first export\n",
    "\n",
    "print(\"CSV sources:\")\n",
    "for p, y in DATA_FILES:\n",
    "    print(f\" - {y}: {p}\")\n",
    "print(\"Figures dir:\", FIG_DIR.resolve())\n",
    "print(\"Derived-data dir:\", DATA_DIR.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c96e707",
   "metadata": {},
   "source": [
    "## 1) Load & union the data\n",
    "\n",
    "- Load both CSVs and **concatenate** into one table.\n",
    "- Parse `InvoiceDate` to datetime (keeps time).\n",
    "- Add `YearSheet` column.\n",
    "- **Checks:** per-file row counts; combined rows; min/max `InvoiceDate`; identical column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f25acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_union(data_files):\n",
    "    frames, per_file_counts = [], []\n",
    "    cols_ref = None\n",
    "    for path, label in data_files:\n",
    "        df_i = pd.read_csv(path)\n",
    "        df_i[\"YearSheet\"] = label\n",
    "        df_i.columns = [c.strip() for c in df_i.columns]  # harmonize\n",
    "        if cols_ref is None:\n",
    "            cols_ref = df_i.columns.tolist()\n",
    "        else:\n",
    "            assert df_i.columns.tolist() == cols_ref, \"Column names mismatch between files.\"\n",
    "        per_file_counts.append((label, len(df_i)))\n",
    "        frames.append(df_i)\n",
    "    df = pd.concat(frames, ignore_index=True)\n",
    "    return df, per_file_counts, cols_ref\n",
    "\n",
    "raw_df, per_file_counts, cols_ref = load_and_union(DATA_FILES)\n",
    "\n",
    "print(\"Per-file row counts:\", per_file_counts)\n",
    "print(\"Combined rows:\", len(raw_df))\n",
    "print(\"Columns:\", cols_ref)\n",
    "\n",
    "# Parse datetime\n",
    "raw_df[\"InvoiceDate\"] = pd.to_datetime(raw_df[\"InvoiceDate\"], errors=\"coerce\")\n",
    "print(\"Earliest date:\", raw_df[\"InvoiceDate\"].min())\n",
    "print(\"Latest date:\", raw_df[\"InvoiceDate\"].max())\n",
    "\n",
    "raw_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0846b1d5",
   "metadata": {},
   "source": [
    "## 2) Basic cleaning\n",
    "\n",
    "- Drop **exact duplicate** rows.\n",
    "- Report % **missing `Customer ID`** (keep for product/country/time EDA; exclude from customer-level views).\n",
    "- Create: `Is_Return = Quantity < 0`, `Revenue = Quantity * Price`.\n",
    "- Build **sales subset** for rankings/AOV: `Price > 0` & `Quantity > 0`.\n",
    "- **Checks:** counts of returns; assert no `Price<=0` or `Qty<=0` in **sales subset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f865be40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_df.copy()\n",
    "\n",
    "# Drop exact duplicates\n",
    "before = len(df)\n",
    "df = df.drop_duplicates()\n",
    "print(f\"Dropped duplicates: {before - len(df)} | Remaining: {len(df)}\")\n",
    "\n",
    "# Missing Customer ID\n",
    "missing_pct = df[\"Customer ID\"].isna().mean() * 100\n",
    "print(f\"%% rows missing Customer ID: {missing_pct:.2f}%%\")\n",
    "\n",
    "# Derived flags\n",
    "df[\"Is_Return\"] = df[\"Quantity\"] < 0\n",
    "df[\"Revenue\"]   = df[\"Quantity\"] * df[\"Price\"]\n",
    "\n",
    "# Sales subset for gross metrics\n",
    "sales_subset = df[(df[\"Price\"] > 0) & (df[\"Quantity\"] > 0)].copy()\n",
    "\n",
    "print(\"Return counts:\")\n",
    "print(df[\"Is_Return\"].value_counts(dropna=False))\n",
    "\n",
    "assert (sales_subset[\"Price\"] <= 0).sum() == 0, \"Found Price<=0 in sales subset.\"\n",
    "assert (sales_subset[\"Quantity\"] <= 0).sum() == 0, \"Found non-positive Quantity in sales subset.\"\n",
    "print(\"Sales subset rows:\", len(sales_subset))\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab708c7",
   "metadata": {},
   "source": [
    "## 3) Time features\n",
    "\n",
    "Derive from `InvoiceDate`:\n",
    "- `Year`, `Quarter`, `Month`, `DayOfWeek` (0=Mon), `Hour`\n",
    "- `InvoiceDateFloorMonth` (month start) for rollups\n",
    "\n",
    "**Checks:** value counts for `Hour` and `DayOfWeek`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612655d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Year\"]   = df[\"InvoiceDate\"].dt.year\n",
    "df[\"Quarter\"]= df[\"InvoiceDate\"].dt.quarter\n",
    "df[\"Month\"]  = df[\"InvoiceDate\"].dt.month\n",
    "df[\"DayOfWeek\"] = df[\"InvoiceDate\"].dt.dayofweek\n",
    "df[\"Hour\"]   = df[\"InvoiceDate\"].dt.hour\n",
    "df[\"InvoiceDateFloorMonth\"] = df[\"InvoiceDate\"].dt.to_period(\"M\").dt.to_timestamp()\n",
    "\n",
    "sales_subset[\"InvoiceDateFloorMonth\"] = sales_subset[\"InvoiceDate\"].dt.to_period(\"M\").dt.to_timestamp()\n",
    "\n",
    "print(\"Hour counts (first 10 hours):\")\n",
    "print(df[\"Hour\"].value_counts().sort_index().head(10))\n",
    "\n",
    "print(\"\\nDayOfWeek counts (0=Mon):\")\n",
    "print(df[\"DayOfWeek\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996d755b",
   "metadata": {},
   "source": [
    "## 4) Orders & customers\n",
    "\n",
    "- Each unique `Invoice` = one **order**.\n",
    "- Compute **AOV**, **items per order** (total positive qty per invoice), **orders per customer**.\n",
    "- Tag **New vs Repeat** per `YearSheet` (first purchase **month** logic).\n",
    "- **Outputs:** table of `n_orders`, `n_customers`, `AOV`, `items_per_order` per `YearSheet`; chart of **New vs Repeat** revenue share."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e323f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order metrics (on sales subset)\n",
    "order_rev = sales_subset.groupby([\"YearSheet\",\"Invoice\"], as_index=False)[\"Revenue\"].sum()\n",
    "order_qty = sales_subset.groupby([\"YearSheet\",\"Invoice\"], as_index=False)[\"Quantity\"].sum()\n",
    "orders = order_rev.merge(order_qty, on=[\"YearSheet\",\"Invoice\"], how=\"left\")                  .rename(columns={\"Revenue\":\"OrderRevenue\",\"Quantity\":\"OrderItems\"})\n",
    "\n",
    "summary_orders = orders.groupby(\"YearSheet\").agg(\n",
    "    n_orders=(\"Invoice\",\"nunique\"),\n",
    "    AOV=(\"OrderRevenue\",\"mean\"),\n",
    "    items_per_order=(\"OrderItems\",\"mean\")\n",
    ").reset_index()\n",
    "\n",
    "# Customers with IDs\n",
    "cust_sales = sales_subset.dropna(subset=[\"Customer ID\"]).copy()\n",
    "cust_sales[\"Customer ID\"] = cust_sales[\"Customer ID\"].astype(int)\n",
    "n_customers = (cust_sales.groupby(\"YearSheet\")[\"Customer ID\"].nunique()\n",
    "               .rename(\"n_customers\")).reset_index()\n",
    "\n",
    "summary_orders = summary_orders.merge(n_customers, on=\"YearSheet\", how=\"left\")\n",
    "summary_orders[[\"YearSheet\",\"n_orders\",\"n_customers\",\"AOV\",\"items_per_order\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be826434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New vs Repeat tagging per YearSheet (first purchase month)\n",
    "def tag_new_repeat(cdf):\n",
    "    first_month = cdf.groupby(\"Customer ID\")[\"InvoiceDateFloorMonth\"].min().rename(\"FirstMonth\")\n",
    "    tagged = cdf.join(first_month, on=\"Customer ID\")\n",
    "    tagged[\"CustType\"] = np.where(tagged[\"InvoiceDateFloorMonth\"]==tagged[\"FirstMonth\"], \"New\", \"Repeat\")\n",
    "    return tagged\n",
    "\n",
    "cust_month = (cust_sales[[\"YearSheet\",\"Customer ID\",\"InvoiceDate\",\"InvoiceDateFloorMonth\",\"Revenue\"]]).copy()\n",
    "nr = []\n",
    "for ys, sub in cust_month.groupby(\"YearSheet\"):\n",
    "    nr.append(tag_new_repeat(sub).assign(YearSheet=ys))\n",
    "nr = pd.concat(nr, ignore_index=True)\n",
    "\n",
    "nr_rev = nr.groupby([\"YearSheet\",\"CustType\"])[\"Revenue\"].sum().reset_index()\n",
    "nr_rev[\"Share\"] = nr_rev[\"Revenue\"] / nr_rev.groupby(\"YearSheet\")[\"Revenue\"].transform(\"sum\")\n",
    "nr_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07951391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: New vs Repeat revenue share\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "for i, ys in enumerate(sorted(nr_rev[\"YearSheet\"].unique())):\n",
    "    part = nr_rev[nr_rev[\"YearSheet\"]==ys].sort_values(\"CustType\")\n",
    "    ax.bar([i-0.2, i+0.2], part[\"Share\"].values)\n",
    "ax.set_xticks(range(len(sorted(nr_rev[\"YearSheet\"].unique()))))\n",
    "ax.set_xticklabels(sorted(nr_rev[\"YearSheet\"].unique()))\n",
    "ax.set_ylabel(\"Revenue Share\")\n",
    "ax.set_title(\"New vs Repeat Revenue Share by YearSheet\")\n",
    "plt.tight_layout()\n",
    "out = FIG_DIR/\"new_vs_repeat_share.png\"\n",
    "plt.savefig(out); plt.show(); print(\"Saved:\", out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8557a110",
   "metadata": {},
   "source": [
    "## 5) Product & country profiles\n",
    "\n",
    "- **Top 10 products by revenue** (gross: exclude returns when ranking).\n",
    "- **Top 10 countries by revenue** and **UK vs Rest-of-World** share.\n",
    "- **Return-prone products**: `return_rate = abs(negative Qty)/total Qty`, **threshold ≥200 units**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e200c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 products (gross revenue)\n",
    "prod = (sales_subset.groupby([\"StockCode\",\"Description\"], as_index=False)\n",
    "        .agg(Revenue=(\"Revenue\",\"sum\"), Quantity=(\"Quantity\",\"sum\")))\n",
    "top10_products = prod.sort_values(\"Revenue\", ascending=False).head(10).reset_index(drop=True)\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "top10_products.to_csv(DATA_DIR/\"top10_products_by_revenue.csv\", index=False)\n",
    "top10_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302ba5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 countries (gross revenue)\n",
    "cty = (sales_subset.groupby(\"Country\", as_index=False)\n",
    "       .agg(Revenue=(\"Revenue\",\"sum\")))\n",
    "cty[\"Share\"] = cty[\"Revenue\"]/cty[\"Revenue\"].sum()\n",
    "top10_countries = cty.sort_values(\"Revenue\", ascending=False).head(10).reset_index(drop=True)\n",
    "top10_countries.to_csv(DATA_DIR/\"top10_countries_by_revenue.csv\", index=False)\n",
    "top10_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1f8e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UK vs Rest-of-World chart\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "uk_rev = cty.loc[cty[\"Country\"]==\"United Kingdom\",\"Revenue\"].sum()\n",
    "row_rev = cty.loc[cty[\"Country\"]!=\"United Kingdom\",\"Revenue\"].sum()\n",
    "fig, ax = plt.subplots(figsize=(5,4))\n",
    "ax.bar([\"United Kingdom\",\"Rest of World\"], [uk_rev, row_rev])\n",
    "ax.set_ylabel(\"Gross Revenue\")\n",
    "ax.set_title(\"UK vs Rest-of-World Revenue Share (Gross, Sales Subset)\")\n",
    "plt.tight_layout()\n",
    "out = FIG_DIR/\"uk_vs_row_gross_revenue.png\"\n",
    "plt.savefig(out); plt.show(); print(\"Saved:\", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d599cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return-prone products (units threshold)\n",
    "sku_total = df.groupby([\"StockCode\",\"Description\"])[\"Quantity\"].agg(total_qty=lambda s: s.abs().sum())\n",
    "sku_ret   = df[df[\"Quantity\"]<0].groupby([\"StockCode\",\"Description\"])[\"Quantity\"].agg(returns_qty=lambda s: s.abs().sum())\n",
    "ret = pd.concat([sku_total, sku_ret], axis=1).fillna(0).reset_index()\n",
    "ret[\"return_rate\"] = np.where(ret[\"total_qty\"]>0, ret[\"returns_qty\"]/ret[\"total_qty\"], 0.0)\n",
    "ret = ret[ret[\"total_qty\"]>=200].sort_values(\"return_rate\", ascending=False)\n",
    "ret_top = ret.head(10).reset_index(drop=True)\n",
    "ret_top.to_csv(DATA_DIR/\"return_prone_products.csv\", index=False)\n",
    "ret_top"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a1b0e7",
   "metadata": {},
   "source": [
    "## 6) Time-series EDA\n",
    "\n",
    "- **Monthly net revenue** line (returns included as negatives), annotated if needed.\n",
    "- **Seasonality**: average net revenue by **Month (1–12)**.\n",
    "- *(Optional)* **Hourly** average revenue (gross, from sales subset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de03351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly net revenue\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "monthly_net = (df.set_index(\"InvoiceDate\").resample(\"MS\")[\"Revenue\"].sum().reset_index())\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ax.plot(monthly_net[\"InvoiceDate\"], monthly_net[\"Revenue\"])\n",
    "ax.set_title(\"Monthly Net Revenue (Returns Included)\")\n",
    "ax.set_xlabel(\"Month\"); ax.set_ylabel(\"Net Revenue\")\n",
    "plt.tight_layout(); out = FIG_DIR/\"monthly_net_revenue.png\"\n",
    "plt.savefig(out); plt.show(); print(\"Saved:\", out)\n",
    "\n",
    "# Seasonality by month number\n",
    "monthly_net[\"MonthNum\"] = monthly_net[\"InvoiceDate\"].dt.month\n",
    "seasonality = monthly_net.groupby(\"MonthNum\")[\"Revenue\"].mean().reset_index()\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.bar(seasonality[\"MonthNum\"], seasonality[\"Revenue\"])\n",
    "ax.set_title(\"Average Net Revenue by Month (Seasonality)\")\n",
    "ax.set_xlabel(\"Month (1–12)\"); ax.set_ylabel(\"Average Net Revenue\")\n",
    "plt.tight_layout(); out = FIG_DIR/\"seasonality_avg_net_revenue_by_month.png\"\n",
    "plt.savefig(out); plt.show(); print(\"Saved:\", out)\n",
    "\n",
    "# Optional hourly pattern (gross, sales subset)\n",
    "hourly = sales_subset.groupby(\"Hour\")[\"Revenue\"].mean().reset_index()\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(hourly[\"Hour\"], hourly[\"Revenue\"])\n",
    "ax.set_title(\"Average Gross Revenue by Hour (Sales Subset)\")\n",
    "ax.set_xlabel(\"Hour of Day\"); ax.set_ylabel(\"Average Revenue\")\n",
    "plt.tight_layout(); out = FIG_DIR/\"hourly_avg_gross_revenue_sales_subset.png\"\n",
    "plt.savefig(out); plt.show(); print(\"Saved (optional):\", out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b325cc",
   "metadata": {},
   "source": [
    "## 7) Returns analysis\n",
    "\n",
    "- **Overall return rate** (units): `sum(negative Qty)/sum(abs(Qty))`.\n",
    "- **Revenue impact**: `sum(negative Revenue)` vs **net** revenue.\n",
    "- Breakouts **by Country** and for **Top 10 products**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48efff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_units = df.loc[df[\"Quantity\"]<0,\"Quantity\"].abs().sum()\n",
    "tot_units = df[\"Quantity\"].abs().sum()\n",
    "overall_return_rate = (neg_units/tot_units) if tot_units>0 else np.nan\n",
    "neg_revenue = df.loc[df[\"Revenue\"]<0,\"Revenue\"].sum()\n",
    "net_revenue = df[\"Revenue\"].sum()\n",
    "print(f\"Overall return rate (units): {overall_return_rate:.4f}\")\n",
    "print(f\"Negative revenue (returns): {neg_revenue:,.2f}\")\n",
    "print(f\"Net revenue: {net_revenue:,.2f}\")\n",
    "\n",
    "# By country\n",
    "by_country = df.groupby(\"Country\").agg(\n",
    "    neg_units=(\"Quantity\", lambda s: s[s<0].abs().sum()),\n",
    "    total_units=(\"Quantity\", lambda s: s.abs().sum()),\n",
    "    neg_revenue=(\"Revenue\", lambda s: s[s<0].sum()),\n",
    "    net_revenue=(\"Revenue\",\"sum\")\n",
    ").reset_index()\n",
    "by_country[\"return_rate\"] = np.where(by_country[\"total_units\"]>0, by_country[\"neg_units\"]/by_country[\"total_units\"], np.nan)\n",
    "by_country = by_country.sort_values(\"return_rate\", ascending=False)\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "by_country.to_csv(DATA_DIR/\"returns_by_country.csv\", index=False)\n",
    "by_country.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db10aaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the previously computed top-10 products\n",
    "top10_codes = set(top10_products[\"StockCode\"].tolist())\n",
    "by_product = df[df[\"StockCode\"].isin(top10_codes)].groupby([\"StockCode\",\"Description\"]).agg(\n",
    "    neg_units=(\"Quantity\", lambda s: s[s<0].abs().sum()),\n",
    "    total_units=(\"Quantity\", lambda s: s.abs().sum()),\n",
    "    neg_revenue=(\"Revenue\", lambda s: s[s<0].sum()),\n",
    "    net_revenue=(\"Revenue\",\"sum\")\n",
    ").reset_index()\n",
    "by_product[\"return_rate\"] = np.where(by_product[\"total_units\"]>0, by_product[\"neg_units\"]/by_product[\"total_units\"], np.nan)\n",
    "by_product = by_product.sort_values(\"return_rate\", ascending=False)\n",
    "by_product.to_csv(DATA_DIR/\"returns_for_top10_products.csv\", index=False)\n",
    "by_product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ec79bc",
   "metadata": {},
   "source": [
    "## 8) Customer-level snapshots (RFM-lite)\n",
    "\n",
    "Where `Customer ID` is present:\n",
    "- **R:** days since last purchase (at dataset end)\n",
    "- **F:** number of invoices\n",
    "- **M:** total net revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf32cf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_df = df.dropna(subset=[\"Customer ID\"]).copy()\n",
    "cust_df[\"Customer ID\"] = cust_df[\"Customer ID\"].astype(int)\n",
    "\n",
    "dataset_end = df[\"InvoiceDate\"].max()\n",
    "last_purchase = cust_df.groupby(\"Customer ID\")[\"InvoiceDate\"].max()\n",
    "recency_days = (dataset_end - last_purchase).dt.days.rename(\"RecencyDays\")\n",
    "frequency = cust_df.groupby(\"Customer ID\")[\"Invoice\"].nunique().rename(\"Frequency\")\n",
    "monetary = cust_df.groupby(\"Customer ID\")[\"Revenue\"].sum().rename(\"Monetary\")\n",
    "rfm = pd.concat([recency_days, frequency, monetary], axis=1).reset_index()\n",
    "\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "rfm.to_csv(DATA_DIR/\"rfm_snapshot.csv\", index=False)\n",
    "\n",
    "# Histograms (separate plots)\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.hist(rfm[\"RecencyDays\"].dropna(), bins=30)\n",
    "ax.set_title(\"Recency (days since last purchase)\"); ax.set_xlabel(\"Days\"); ax.set_ylabel(\"Customers\")\n",
    "plt.tight_layout(); out = FIG_DIR/\"rfm_hist_recency.png\"; plt.savefig(out); plt.show(); print(\"Saved:\", out)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.hist(rfm[\"Frequency\"].dropna(), bins=30)\n",
    "ax.set_title(\"Frequency (# of invoices)\"); ax.set_xlabel(\"Invoices\"); ax.set_ylabel(\"Customers\")\n",
    "plt.tight_layout(); out = FIG_DIR/\"rfm_hist_frequency.png\"; plt.savefig(out); plt.show(); print(\"Saved:\", out)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.hist(rfm[\"Monetary\"].dropna(), bins=30)\n",
    "ax.set_title(\"Monetary (net revenue per customer)\"); ax.set_xlabel(\"Revenue\"); ax.set_ylabel(\"Customers\")\n",
    "plt.tight_layout(); out = FIG_DIR/\"rfm_hist_monetary.png\"; plt.savefig(out); plt.show(); print(\"Saved:\", out)\n",
    "\n",
    "print(\"RFM five-number summaries:\")\n",
    "rfm.describe(percentiles=[0.25,0.5,0.75]).T[[\"min\",\"25%\",\"50%\",\"75%\",\"max\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c65ae8",
   "metadata": {},
   "source": [
    "## 9) Synthesis & recommendations (for your manuscript)\n",
    "\n",
    "Write **5–7 insights**, each supported by a figure/table above, and propose **3 actions** the retailer can test next month.\n",
    "- Example actions: peak-hour staffing/stocking, SKU-specific return reduction, segmented bundles for repeaters/new customers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
